{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Ans 1 K-Nearest Neighbors, commonly known as KNN, is a simple yet powerful supervised machine learning algorithm used for both classification and regression tasks. It is an instance-based or lazy learning method, which means it does not build an explicit model during the training phase. Instead, it memorizes the entire training dataset and makes predictions only when new data points are given. The core idea of KNN is based on the assumption that similar data points exist close to each other in the feature space. Therefore, the algorithm relies on measuring distances between data points to find the nearest neighbors.\n",
        "\n",
        "KNN works by selecting a value of K, which represents the number of closest training samples to consider. When a new data point is introduced, the algorithm computes the distance between this point and all points in the training dataset. Common distance metrics include Euclidean distance, Manhattan distance and Minkowski distance. After calculating these distances, KNN identifies the K nearest data points as neighbors. The choice of K is crucial because a very small K may make the model sensitive to noise, while a very large K may smooth out important patterns.\n",
        "\n",
        "In classification problems, KNN predicts the class label of a new data point based on the majority voting among its K nearest neighbors. If most of the neighbors belong to a particular class, the new point is assigned to that class. For example, if the nearest five neighbors consist of three samples from Class A and two from Class B, the algorithm will classify the new point as Class A. This approach works well when classes are well separated and the dataset is not too large.\n",
        "\n",
        "In regression problems, KNN predicts a continuous value instead of a class label. Instead of majority voting, the algorithm takes the average or mean of the output values of the K nearest neighbors. For example, if the algorithm is predicting house prices, the predicted price for a new house will be the average price of the nearest K houses in the dataset. This averaging makes the prediction smooth and reduces the effect of outliers if K is chosen properly.\n",
        "\n",
        "KNN has several advantages. It is simple to understand, easy to implement and effective in many real-world applications. It makes no strong assumptions about the underlying data distribution, making it useful for complex or non-linear decision boundaries. However, it also has limitations. KNN can be computationally expensive for large datasets because it requires calculating the distance to all training samples for every prediction. It can also be affected by irrelevant or unscaled features, which is why feature scaling techniques like normalization or standardization are often used.\n",
        "\n",
        "In summary, KNN is a versatile and intuitive algorithm that classifies or predicts values based on proximity to neighboring points. It relies on distance calculations, majority voting for classification and averaging for regression. With proper choice of K and well-prepared data, KNN can perform strongly in many machine learning tasks.\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans 2 The Curse of Dimensionality refers to a set of problems and challenges that arise when data is represented in a high number of features or dimensions. As the number of dimensions increases, the data becomes sparse, distances between points become less meaningful and traditional distance-based algorithms like KNN struggle to perform effectively. In simple terms, when the number of features grows, the volume of the data space increases so rapidly that available data becomes insufficient to properly represent the relationships within that space. This leads to poor model performance and increased computational cost.\n",
        "\n",
        "In low-dimensional spaces, points tend to be naturally clustered and the concept of closeness makes sense. However, in high-dimensional spaces, all points start appearing equally far from one another. This makes it difficult for KNN to identify true nearest neighbors because the difference between the nearest and farthest points becomes very small. When distance loses its discriminative power, KNN predictions become unreliable. Since KNN depends completely on accurately finding the closest neighbors, any distortion in distance measurements directly reduces its accuracy.\n",
        "\n",
        "The Curse of Dimensionality also causes sparsity of data. When dimensions increase, exponentially more data is needed to maintain the same density of samples. However, in most real-world problems, collecting such huge datasets is not practical. As a result, data points become scattered widely, meaning that each region in the space has very few nearby points. For KNN, this leads to unstable predictions because the algorithm may end up selecting neighbors that are not truly similar. This problem becomes even more severe when the dataset contains irrelevant or redundant features, as they add unnecessary dimensions and dilute the importance of the informative ones.\n",
        "\n",
        "Another effect is the computational burden. KNN is a lazy learner, meaning it performs all computations at prediction time. With more dimensions, the algorithm has to calculate distances using many features for every data point in the training set. This increases both memory usage and computation time. For large datasets, this makes KNN slow and less efficient. Additionally, high-dimensional distance calculations can introduce numerical instability, further affecting prediction quality.\n",
        "\n",
        "To reduce the effects of the Curse of Dimensionality, techniques such as feature selection, feature extraction and dimensionality reduction are commonly applied. Methods like Principal Component Analysis, removing irrelevant features or applying domain knowledge help in reducing the number of dimensions while preserving important information. By lowering the dimensionality, KNN can better differentiate between similar and dissimilar points, improving accuracy and reducing computation time.\n",
        "\n",
        "In conclusion, the Curse of Dimensionality describes the difficulties that arise when dealing with high-dimensional data. For KNN, it weakens the meaning of distance, increases sparsity, raises computational costs and leads to inaccurate predictions. Understanding and addressing this phenomenon is essential when applying KNN to real-world datasets with many features.\n",
        "\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Ans 3 Principal Component Analysis, commonly known as PCA, is a statistical technique used for dimensionality reduction while preserving as much variability in the data as possible. It transforms the original high-dimensional data into a new set of variables called principal components. These components are uncorrelated and are ordered in such a way that the first principal component captures the maximum possible variance in the dataset, the second captures the next highest variance and so on. PCA works by identifying the directions, known as eigenvectors, in which the data varies the most and projecting the data onto these directions. This transformation reduces the number of features while retaining the core structure and patterns of the dataset.\n",
        "\n",
        "PCA is particularly useful when dealing with high-dimensional data where many features may be correlated or redundant. By converting correlated features into a smaller number of uncorrelated components, PCA simplifies the dataset without losing important information. This not only makes the data easier to visualize but also improves the efficiency and performance of machine learning algorithms by removing noise and reducing computational complexity. PCA achieves this through mathematical operations involving covariance matrices, eigenvalues and eigenvectors, which together help identify the principal directions of variance.\n",
        "\n",
        "Although PCA reduces the number of dimensions, it does so by creating new features that are combinations of the original ones. These new features are often difficult to interpret because they do not correspond to the original input variables directly. This characteristic distinguishes PCA from other dimensionality-reduction methods that simply remove or filter features.\n",
        "\n",
        "PCA is different from feature selection in several important ways. Feature selection refers to the process of choosing a subset of the original features that are most relevant to the target variable or provide the most useful information. In feature selection, no new features are created. Instead, it retains the original features that contribute the most to model performance and discards the irrelevant or redundant ones. The selected features remain interpretable and maintain their original meaning.\n",
        "\n",
        "In contrast, PCA performs feature extraction rather than selection. It creates completely new features by combining existing ones through linear transformations. These principal components are mathematically optimized to capture maximum variance but they do not represent individual measurable attributes from the dataset. As a result, PCA focuses on transforming the feature space, while feature selection focuses on identifying a smaller set of original features.\n",
        "\n",
        "Another difference is that PCA is an unsupervised technique, meaning it does not consider the target variable while reducing the dimensionality. It focuses purely on the structure and variance within the feature space. Feature selection, on the other hand, can be either supervised or unsupervised. In supervised feature selection, the target label guides the process so that the selected features improve prediction accuracy.\n",
        "\n",
        "In summary, PCA is a dimensionality-reduction technique that transforms data into new uncorrelated components capturing maximum variance. It differs from feature selection because PCA creates new transformed features, while feature selection simply retains a subset of the original features without modifying them. PCA is useful for simplifying high-dimensional datasets, while feature selection is important for improving model interpretability and reducing complexity.\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans 4 In Principal Component Analysis, eigenvalues and eigenvectors are fundamental mathematical concepts that determine how the data is transformed into principal components. They arise from the decomposition of the covariance matrix of the dataset. The covariance matrix captures the relationships and correlations among the features. When this matrix is decomposed, it produces eigenvalues and their corresponding eigenvectors, which together help identify the principal directions of maximum variance in the data.\n",
        "\n",
        "An eigenvector represents a direction in the feature space along which the data varies the most. Each eigenvector defines one of the principal components in PCA. These eigenvectors are orthogonal to one another, meaning each principal component captures a unique pattern of variation that does not overlap with the others. The eigenvectors essentially act as new axes onto which the data is projected. These axes are chosen so they describe the structure of the data more efficiently than the original features.\n",
        "\n",
        "Eigenvalues indicate the amount of variance captured by each eigenvector. A larger eigenvalue means that its corresponding eigenvector represents a direction with greater variability in the dataset. In PCA, the eigenvalues are sorted in descending order, and the eigenvectors with the largest eigenvalues form the first few principal components. These principal components capture most of the important structure in the data. Components with very small eigenvalues contribute little information and are often discarded during dimensionality reduction.\n",
        "\n",
        "These concepts are important in PCA because they determine how much information each principal component carries. If a principal component has a high eigenvalue, it means this new axis explains a significant portion of the overall variance, making it highly valuable for simplifying the dataset. By selecting the components with the highest eigenvalues, PCA reduces dimensionality while preserving the essential structure and patterns present in the original data.\n",
        "\n",
        "Eigenvalues and eigenvectors also help remove noise and redundancy. Since correlated features contribute to the same directions of variance, their information is combined into fewer principal components. This reduces duplication of information and helps create a more compact representation of the dataset. The process also improves computational efficiency for machine learning algorithms and can enhance model performance by focusing on the most informative parts of the data.\n",
        "\n",
        "Another reason they are important is that eigenvalues and eigenvectors make PCA mathematically sound and objective. Instead of arbitrarily choosing features to keep or discard, PCA uses linear algebra to find the most meaningful directions based strictly on the data’s statistical properties. This ensures that dimensionality reduction is carried out in an optimal way, maximizing variance while minimizing information loss.\n",
        "\n",
        "In summary, eigenvalues and eigenvectors in PCA define the core structure of the transformation. Eigenvectors represent the directions of maximum variance, and eigenvalues measure the significance of each direction. They are essential because they determine which principal components should be kept, how much variance they capture and how effectively PCA can reduce dimensionality while preserving important information.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans 5 KNN and PCA complement each other effectively when used together in a single pipeline because they address different challenges found in high-dimensional datasets. PCA is a dimensionality reduction technique that transforms the original data into a smaller set of uncorrelated components while preserving maximum variance. KNN, on the other hand, is a distance-based algorithm that relies heavily on meaningful distance calculations between data points for accurate predictions. When these two methods are combined, PCA helps prepare the data in a way that improves the performance, accuracy and efficiency of KNN.\n",
        "\n",
        "One major issue that KNN faces is the Curse of Dimensionality. When the number of features increases, distances between data points become less meaningful, and KNN struggles to accurately identify the nearest neighbors. PCA helps solve this problem by reducing the number of dimensions and retaining only the most informative components. By projecting the data into a lower-dimensional space, PCA ensures that distance calculations performed by KNN become more reliable and discriminative, leading to better classification or regression results.\n",
        "\n",
        "Another advantage of using PCA before KNN is noise reduction. High-dimensional datasets often contain redundant or irrelevant features that do not contribute useful information to the prediction. These noisy features can distort distance calculations and mislead the KNN algorithm. PCA filters out such noise by focusing only on the directions that explain the maximum variance in the data. This results in a cleaner and more compact dataset, enabling KNN to operate on features that truly matter.\n",
        "\n",
        "The combination of PCA and KNN also improves computational efficiency. KNN is a lazy learner, meaning it performs most of its computation during prediction by calculating distances to all training samples. When the number of features is very high, this process becomes slow and computationally expensive. PCA reduces the feature space, lowering the number of calculations KNN must perform and speeding up both training and prediction. This makes the pipeline particularly useful for large datasets or real-time applications.\n",
        "\n",
        "Using PCA with KNN can also enhance model generalization. When data is high-dimensional, models can easily overfit due to the presence of noise and unnecessary complexity. By reducing dimensionality, PCA simplifies the data and highlights its fundamental structure. This helps KNN make more robust predictions on unseen data and reduces the chances of overfitting.\n",
        "\n",
        "Additionally, PCA transforms features into uncorrelated components. In many real-world datasets, original features are highly correlated, which may cause redundant influence on the distance calculations in KNN. By converting these correlated features into independent principal components, PCA ensures that no single pattern dominates the prediction process. This improves the fairness and effectiveness of KNN in handling the transformed feature space.\n",
        "\n",
        "In summary, KNN and PCA work well together because PCA reduces dimensionality, removes noise, decorrelates features and improves the efficiency and reliability of distance calculations, while KNN performs accurate prediction on this transformed and optimized data. When combined in a pipeline, PCA enhances the accuracy, speed and robustness of the KNN algorithm, making the overall machine learning model more effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "LTQIIpZ2j7HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "-5NNyCwrlwSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(\"Accuracy without Feature Scaling:\", accuracy_no_scaling)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy with Feature Scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyYkrBOFl5Ku",
        "outputId": "15671aae-7c24-4f84-86aa-d1ab74d35f96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Feature Scaling: 0.7407407407407407\n",
            "Accuracy with Feature Scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "9uw1SbyGmJbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxqfBoYUmKkD",
        "outputId": "e7478473-7fa2-4c8e-cd16-1dd966984634"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "EBDpsLzSmZAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(\"Accuracy on Original Dataset (scaled):\", accuracy_original)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"Accuracy on PCA-transformed Dataset (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVWPKu8hmZ8_",
        "outputId": "2878e2c3-1a8c-48ae-f24a-f964edaba31f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Dataset (scaled): 0.9629629629629629\n",
            "Accuracy on PCA-transformed Dataset (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "xcx5GTzbms_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# KNN with Euclidean Distance (default: metric='minkowski', p=2)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(\"Accuracy with Euclidean Distance:\", accuracy_euclidean)\n",
        "\n",
        "\n",
        "# KNN with Manhattan Distance (metric='minkowski', p=1)\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(\"Accuracy with Manhattan Distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqoX6EddnQeN",
        "outputId": "3c9bfc48-59da-4a97-a628-e98703dad6d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9629629629629629\n",
            "Accuracy with Manhattan Distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "XLGy0R_dneMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis is used to reduce dimensionality by finding orthogonal directions (principal components) that capture maximal variance in the feature space. For high-dimensional gene expression data where the number of features far exceeds the number of samples, PCA reduces noise and redundancy by projecting the data onto a lower-dimensional subspace formed by the top principal components. Before PCA, the features must be standardized because gene expression features often have different scales; standardization ensures PCA finds directions based on variance that are comparable across features. In practice, PCA will convert thousands of gene features into a modest number of components that retain the important patterns in the data while removing random measurement noise and correlated features.\n",
        "\n",
        "Deciding how many principal components to keep should balance information retention, model complexity, and generalization. A common approach is to inspect the cumulative explained variance ratio and pick the smallest number of components that explains a large fraction of total variance (for example 90–99 percent), but that rule-of-thumb can be suboptimal for downstream classification. A more robust approach is to evaluate downstream model performance as a function of retained components using cross-validation: try several variance thresholds or fixed component counts and choose the number that yields the best cross-validated performance while keeping the model simple enough to avoid overfitting. Visual methods such as a scree plot or elbow detection help identify diminishing returns in explained variance, and domain knowledge (e.g., known biology suggesting how many latent factors are plausible) can be incorporated.\n",
        "\n",
        "After reducing dimensionality with PCA, K-Nearest Neighbors can be applied on the PCA-transformed features. Because PCA components are orthogonal and typically far fewer than the original features, distance calculations used by KNN become more meaningful and less noisy. The recommended pipeline sequence is: standardize features, apply PCA with a chosen n_components, then fit KNN. Tune KNN hyperparameters such as number of neighbors, distance metric (Euclidean or Manhattan), and weighting using nested cross-validation or grid search with stratified folds to avoid overfitting small-sample idiosyncrasies.\n",
        "\n",
        "Model evaluation must be rigorous given the small sample size. Use stratified k-fold cross-validation to estimate performance and to tune hyperparameters. Report multiple metrics, not only accuracy: precision, recall, F1-score, and (for multiclass) macro-averaged F1, because class imbalance is common in biomedical data. Show confusion matrices so clinicians can see which classes are misclassified and whether the errors are clinically acceptable. If possible, reserve an external held-out validation cohort or use nested cross-validation to get an unbiased estimate of generalization. For probabilistic outputs in binary or one-vs-rest multiclass settings, report ROC-AUC and precision-recall curves. Also compute and report confidence intervals for metrics (for example via bootstrap) and provide interpretable visualizations such as PCA scatter plots colored by class to show separation in the reduced space.\n",
        "\n",
        "To justify this pipeline to stakeholders, emphasize that PCA reduces noise and prevents overfitting by removing thousands of noisy, often correlated gene features, while retaining the dominant biological signals. Explain that KNN on PCA features is simple, transparent, and easy to audit; it makes predictions based on similarity to known patients, which is intuitive to clinicians. Demonstrate robust validation: show cross-validation scores, confusion matrices, and external validation results if available. Explain computational advantages: reduced storage and faster inference, which matter for deployment and reproducibility. Stress that final clinical deployment would require prospective validation and possibly re-training on larger, multi-center cohorts; however, the PCA+KNN pipeline is a sensible, defensible first approach for discovery and small-sample classification because it mitigates overfitting and provides interpretable similarity-based predictions.\n"
      ],
      "metadata": {
        "id": "PcWIGidtn4JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python code: PCA + KNN pipeline for high-dimensional gene expression data\n",
        "# Replace the simulated data with your real dataset (X: samples x genes, y: labels)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Simulate a high-dimensional gene expression dataset: small n, large p\n",
        "X, y = make_classification(\n",
        "    n_samples=80,\n",
        "    n_features=2000,\n",
        "    n_informative=50,\n",
        "    n_redundant=50,\n",
        "    n_classes=3,\n",
        "    n_clusters_per_class=1,\n",
        "    class_sep=1.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train/test split to hold out a test set for final evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Pipeline: Standardize -> PCA -> KNN\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'pca__n_components': [0.90, 0.95, 0.99, 10, 20, 50],  # floats choose by explained variance, ints choose exact components\n",
        "    'knn__n_neighbors': [3, 5, 7, 9],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['minkowski']  # default Euclidean (p=2). You can add 'manhattan' via metric='manhattan' or p=1\n",
        "}\n",
        "\n",
        "# Use stratified 5-fold CV for inner tuning\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid.best_params_)\n",
        "print()\n",
        "print(\"Best CV accuracy (train folds): {:.3f}\".format(grid.best_score_))\n",
        "\n",
        "# Evaluate on held-out test set\n",
        "y_pred = grid.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: {:.3f}\".format(test_acc))\n",
        "print()\n",
        "print(\"Classification report on held-out test set:\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "\n",
        "# For interpretation: show explained variance retained (if PCA chosen as float in best_params)\n",
        "pca_choice = grid.best_params_['pca__n_components']\n",
        "best_pca = grid.best_estimator_.named_steps['pca']\n",
        "if isinstance(pca_choice, float):\n",
        "    total_explained = np.sum(best_pca.explained_variance_ratio_)\n",
        "    print(\"PCA chosen as float explained variance threshold: {:.3f}\".format(pca_choice))\n",
        "    print(\"Actual explained variance captured by selected components: {:.3f}\".format(total_explained))\n",
        "else:\n",
        "    print(\"PCA chosen number of components:\", best_pca.n_components_)\n",
        "    print(\"Explained variance (cumulative) by these components: {:.3f}\".format(np.sum(best_pca.explained_variance_ratio_)))\n",
        "\n",
        "# Quick PCA scatter plot (top 2 components) colored by class to show separation\n",
        "X_train_scaled = grid.best_estimator_.named_steps['scaler'].transform(X_train)\n",
        "X_train_pca2 = PCA(n_components=2).fit_transform(X_train_scaled)  # independent 2-comp view for visualization\n",
        "plt.figure(figsize=(6,5))\n",
        "for cls in np.unique(y_train):\n",
        "    ix = np.where(y_train == cls)\n",
        "    plt.scatter(X_train_pca2[ix,0], X_train_pca2[ix,1], label=f\"class {cls}\", alpha=0.8, s=40)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA (top 2) visualization of training data\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "ysPLrsIPn8ly",
        "outputId": "8775b039-4135-483e-818c-f043bca85cbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Best parameters found by GridSearchCV:\n",
            "{'knn__metric': 'minkowski', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'pca__n_components': 0.99}\n",
            "\n",
            "Best CV accuracy (train folds): 0.578\n",
            "Test set accuracy: 0.688\n",
            "\n",
            "Classification report on held-out test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.667     0.667     0.667         6\n",
            "           1      0.571     0.800     0.667         5\n",
            "           2      1.000     0.600     0.750         5\n",
            "\n",
            "    accuracy                          0.688        16\n",
            "   macro avg      0.746     0.689     0.694        16\n",
            "weighted avg      0.741     0.688     0.693        16\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[4 2 0]\n",
            " [1 4 0]\n",
            " [1 1 3]]\n",
            "PCA chosen as float explained variance threshold: 0.990\n",
            "Actual explained variance captured by selected components: 1.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HElBaGtDnfAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3b_BbVVznQWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWIsH_IPmqH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3CPplkojl3kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8T3sER4jx8b"
      },
      "outputs": [],
      "source": []
    }
  ]
}